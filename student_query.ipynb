{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3838b48b",
   "metadata": {},
   "source": [
    "## Intelligent Student Query Advisor and FAQ Enhancer\n",
    "\n",
    "### Project Outline :\n",
    "\n",
    "* Project Introduction (Overview & Problem Statement)\n",
    "* Data Description (Question & Database Information)\n",
    "* Exploratory Data Analysis (EDA)\n",
    "* Feature Engineering\n",
    "* Machine Learning Models\n",
    "* Model Evaluation\n",
    "* Model Interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21788923",
   "metadata": {},
   "source": [
    "### Overview :\n",
    "This project is designed to build an end-to-end machine learning pipeline that efficiently processes student queries, categorizes them, predicts key service metrics, and uncovers hidden patterns in the data. By integrating both supervised and unsupervised learning techniques along with a robust database, the project aims to improve query resolution, enhance support services, and provide valuable insights for decision-makers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ae818",
   "metadata": {},
   "source": [
    "### Problem Statement :\n",
    "\n",
    "Educational institutions often receive a high volume of student queries covering diverse topics such as academics, finances, and technical issues. Managing and responding to these queries efficiently is crucial for enhancing student satisfaction and administrative productivity. This project aims to develop a machine learning model to classify, predict, and analyze student queries while uncovering trends and improving decision-making processes‚Äîall within a structured Jupyter Notebook environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59024d",
   "metadata": {},
   "source": [
    "**Question :** The University of Excellence is looking to modernize its student support system. The administration wants to implement an automated system that processes student queries submitted via an online portal. Your organization has collected historical data comprising student questions, query categories, response times, satisfaction ratings, and response flag. The aim is to design an end-to-end pipeline that will not only predict relevant answers based on past interactions but also analyze emerging trends from new queries.\n",
    "\n",
    "You're provided with a CSV file capturing student queries along with metadata such as Query ID, Timestamp, Student ID, Query Text, Query Category, Satisfaction Score, Response Time, Resolved Flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling & Processing\n",
    "import pandas as pd # panels for data manipulation and analysis\n",
    "import numpy as np # numerical computing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px  # For interactive visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca982e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, mean_absolute_error, mean_squared_error, r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretability Alternative\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d3656",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffba743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "stu_query = pd.read_csv(\"query_data.csv\")\n",
    "print(\"Dataset Loaded Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_query # Display all the columns in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d646a",
   "metadata": {},
   "source": [
    "### Data Preprocessing (Cleaning the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773647e",
   "metadata": {},
   "source": [
    "Data preprocessing ( cleaning ) is a crucial step to ensure the dataset is clean and ready for analysis and modeling. In this project, preprocessing involved:\n",
    "\n",
    "- **Handling Missing Values and Duplicates:** Checked for any missing data and removed duplicate records to maintain data quality.\n",
    "- **Encoding Categorical Variables:** Converted text-based categories (like `Query_Category` and `Resolved_Flag`) into numerical values using label encoding, making them suitable for machine learning algorithms.\n",
    "- **Feature Scaling:** Standardized numerical features such as `Response_Time` and `Satisfaction_Score` so that all values are on a similar scale, improving model performance.\n",
    "\n",
    "These steps help create a consistent and reliable dataset for further analysis and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and duplicates\n",
    "print(\"Missing Values:\\n\", stu_query.isnull().sum())\n",
    "stu_query.drop_duplicates(inplace=True)\n",
    "print(\"Data cleaned successfully!\")\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = LabelEncoder()\n",
    "stu_query[\"Query_Category\"] = encoder.fit_transform(stu_query[\"Query_Category\"])\n",
    "stu_query[\"Resolved_Flag\"] = encoder.fit_transform(stu_query[\"Resolved_Flag\"])\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "stu_query[[\"Response_Time\", \"Satisfaction_Score\"]] = scaler.fit_transform(stu_query[[\"Response_Time\", \"Satisfaction_Score\"]])\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685f495",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d1a80",
   "metadata": {},
   "source": [
    "#### Interactive Histogram of Satisfaction Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939c3ed",
   "metadata": {},
   "source": [
    "#### What is a Histogram?\n",
    "\n",
    "A histogram is a type of bar plot that shows the distribution of a numerical variable by dividing the data into bins (intervals) and counting how many values fall into each bin. It helps you quickly see patterns such as where most values are concentrated, the spread of the data, and if there are any outliers.\n",
    "\n",
    "- **X-axis:** Represents the range of values (divided into bins).\n",
    "- **Y-axis:** Shows the frequency (count) of values in each bin.\n",
    "\n",
    "Histograms are useful for understanding the overall shape and variability of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(stu_query, x=\"Satisfaction_Score\", nbins=10, title=\"Distribution of Satisfaction Scores\", labels={\"Satisfaction_Score\": \"Satisfaction Score\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5204301a",
   "metadata": {},
   "source": [
    "#### Frequency of Query Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83859b",
   "metadata": {},
   "source": [
    "#### What is a Bar Chart?\n",
    "\n",
    "A bar chart is a graph that uses rectangular bars to represent and compare the frequency, count, or value of different categories. Each bar's length or height is proportional to the value it represents.\n",
    "\n",
    "- **X-axis:** Shows the categories (e.g., types of queries).\n",
    "- **Y-axis:** Shows the values (e.g., number of queries in each category).\n",
    "\n",
    "Bar charts are useful for visualizing and comparing data across different groups or categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stu_query\n",
    "fig = px.bar(df, x=\"Query_Category\", color=\"Query_Category\", title=\"Frequency of Queries per Category\", labels={\"Query_Category\": \"Query Category\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499ee19",
   "metadata": {},
   "source": [
    "#### Box Plot for Response Time by Query Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf151833",
   "metadata": {},
   "source": [
    "#### What is a Box Plot?\n",
    "\n",
    "A box plot (or box-and-whisker plot) is a graphical tool used to display the distribution of a dataset. It shows the median, quartiles, and possible outliers in your data.\n",
    "\n",
    "- **Box:** Represents the middle 50% of the data (from the first quartile (Q1) to the third quartile (Q3)).\n",
    "- **Line inside the box:** Shows the median (middle value).\n",
    "- **Whiskers:** Extend from the box to the smallest and largest values within 1.5 times the interquartile range (IQR).\n",
    "- **Dots (if any):** Indicate outliers, which are values outside the whiskers.\n",
    "\n",
    "Box plots are useful for comparing distributions between groups and spotting outliers or skewness in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Query_Category\", y=\"Response_Time\", data=df)\n",
    "plt.title(\"Response Time Distribution by Query Category\")\n",
    "plt.xlabel(\"Query Category\")\n",
    "plt.ylabel(\"Response Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75259d8c",
   "metadata": {},
   "source": [
    "#### Word Cloud for Query Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a627d",
   "metadata": {},
   "source": [
    "#### What is a Word Cloud?\n",
    "\n",
    "A word cloud is a visual representation of text data where the size of each word indicates how often it appears in the dataset. The more frequently a word is used, the larger and bolder it appears in the cloud. Word clouds help quickly highlight the most common topics or keywords in a collection of text, making it easy to spot trends and important terms at a glance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e498c26",
   "metadata": {},
   "source": [
    "#### What is `imshow`?\n",
    "\n",
    "`imshow` is a function in Matplotlib used to display images or 2D data as a color-coded grid. It is commonly used to visualize matrices, heatmaps, or image data.\n",
    "\n",
    "- **How it works:** Each value in the 2D array is shown as a colored square (pixel).\n",
    "- **Typical uses:** Displaying images, correlation matrices, confusion matrices, or word clouds.\n",
    "\n",
    "**Key Points:**\n",
    "- `imshow` is great for visualizing 2D data.\n",
    "- You can change the color map using the `cmap` parameter.\n",
    "- Add `plt.colorbar()` to show the color scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_texts = \" \".join(df[\"Query_Text\"])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(query_texts)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Student Queries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d00d2",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e591b9",
   "metadata": {},
   "source": [
    "#### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_query.head()  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c788364",
   "metadata": {},
   "source": [
    "#### Classification: Predicting Query Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c135f5",
   "metadata": {},
   "source": [
    "#### What is Classification?\n",
    "\n",
    "Classification is a type of supervised machine learning task where the goal is to predict a category or label for new data based on patterns learned from labeled examples. For example, in this project, we use classification to automatically assign each student query to a specific category (like Academic, Technical, Financial, or General).\n",
    "\n",
    "**Key Points:**\n",
    "- The model learns from historical data with known categories.\n",
    "- It predicts the most likely category for new, unseen queries.\n",
    "- Common algorithms include Random Forest, Decision Trees, and Logistic Regression.\n",
    "\n",
    "**Typical Use Cases:**\n",
    "- Email spam detection (spam or not spam)\n",
    "- Medical diagnosis (disease type)\n",
    "- Customer segmentation (grouping by behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fba01e",
   "metadata": {},
   "source": [
    "#### What is a Heatmap?\n",
    "\n",
    "A heatmap is a graphical representation of data where individual values are shown as colors in a matrix. It is commonly used to visualize the relationship between two variables or to display the frequency of values in a table.\n",
    "\n",
    "- **Rows and columns:** Represent different categories or variables.\n",
    "- **Colors:** Indicate the magnitude or frequency of the values (e.g., darker colors for higher values).\n",
    "\n",
    "Heatmaps are especially useful for visualizing correlation matrices, confusion matrices, or any data where patterns and trends can be spotted through color intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_clf = df[[\"Response_Time\", \"Satisfaction_Score\"]]\n",
    "y_clf = df[\"Query_Category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ac547",
   "metadata": {},
   "source": [
    "#### Regression: Predicting Response Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3b7be",
   "metadata": {},
   "source": [
    "#### What is Regression?\n",
    "\n",
    "Regression is a supervised machine learning technique used to predict a continuous numerical value based on input features. For example, in this project, regression is used to estimate the response time for student queries based on satisfaction scores and other factors.\n",
    "\n",
    "**Key Points:**\n",
    "- Predicts a number (not a category).\n",
    "- Helps understand relationships between variables.\n",
    "- Common algorithms: Linear Regression, Decision Tree Regression.\n",
    "\n",
    "**Typical Use Cases:**\n",
    "- Predicting house prices\n",
    "- Estimating delivery times\n",
    "- Forecasting sales\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "### Common Regression Metrics\n",
    "\n",
    "- **MAE (Mean Absolute Error):**  \n",
    "    Measures the average absolute difference between actual and predicted values.  \n",
    "    Formula:  \n",
    "    $$\n",
    "    \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "    $$\n",
    "    **Use:** Easy to interpret; shows average error in the same units as the target.\n",
    "\n",
    "- **MSE (Mean Squared Error):**  \n",
    "    Measures the average of the squared differences between actual and predicted values.  \n",
    "    Formula:  \n",
    "    $$\n",
    "    \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "    $$\n",
    "    **Use:** Penalizes larger errors more than MAE.\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):**  \n",
    "    The square root of MSE; brings error back to original units.  \n",
    "    Formula:  \n",
    "    $$\n",
    "    \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "    $$\n",
    "    **Use:** Commonly used; easier to interpret than MSE.\n",
    "\n",
    "- **$R^2$ (R-Squared Score):**  \n",
    "    Indicates how well the model explains the variance in the data (ranges from 0 to 1; can be negative if the model is very poor).  \n",
    "    Formula:  \n",
    "    $$\n",
    "    R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "    $$\n",
    "    **Use:** Shows the proportion of variance explained by the model; higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d613afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_reg = df[[\"Satisfaction_Score\"]]\n",
    "y_reg = df[\"Response_Time\"]\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg = reg.predict(X_test_reg)\n",
    "\n",
    "# Model Evaluation\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-Squared Score (R¬≤):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ed5ed",
   "metadata": {},
   "source": [
    "#### Interactive Scatter Plot for Actual vs. Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cddf2e0",
   "metadata": {},
   "source": [
    "#### What is a Scatter Plot?\n",
    "\n",
    "A scatter plot is a type of graph that displays individual data points as dots on a two-dimensional grid. Each dot represents a pair of values‚Äîone on the x-axis and one on the y-axis. Scatter plots are useful for showing the relationship or correlation between two numerical variables.\n",
    "\n",
    "- **X-axis:** Represents one variable (e.g., actual values).\n",
    "- **Y-axis:** Represents another variable (e.g., predicted values).\n",
    "- **Dots:** Each dot shows a single observation.\n",
    "\n",
    "Scatter plots help you see patterns, trends, and possible outliers in your data. If the dots form a line or curve, it suggests a relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Scatter Plot for Actual vs. Predicted\n",
    "fig = px.scatter(x=y_test_reg, y=y_pred_reg, title=\"Actual vs. Predicted Response Time\", labels={\"x\": \"Actual Response Time\", \"y\": \"Predicted Response Time\"})\n",
    "fig.add_shape(type=\"line\", x0=y_test_reg.min(), y0=y_test_reg.min(), x1=y_test_reg.max(), y1=y_test_reg.max(), line=dict(dash=\"dash\", color=\"red\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92507534",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fae9cb",
   "metadata": {},
   "source": [
    "#### Finding Optimal Clusters Using the Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6b84d6",
   "metadata": {},
   "source": [
    "#### What is the \"Optimal Cluster\" in Clustering?\n",
    "\n",
    "The \"optimal cluster\" refers to the best number of groups (clusters) to divide your data into when using clustering algorithms like K-Means. Choosing the right number of clusters helps you find meaningful patterns without overcomplicating the results.\n",
    "\n",
    "- **Why is it important?**  \n",
    "    Too few clusters may mix different types of data together, while too many clusters can split similar data unnecessarily.\n",
    "- **How do we find it?**  \n",
    "    The \"elbow method\" is a common technique: plot the number of clusters against a measure called WCSS (Within-Cluster Sum of Squares). The point where the decrease in WCSS slows down (forms an \"elbow\" shape) is usually the optimal number of clusters.\n",
    "\n",
    "**In summary:**  \n",
    "Finding the optimal cluster helps you group your data in a way that makes the most sense for analysis and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(df[[\"Response_Time\", \"Satisfaction_Score\"]])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "fig = px.line(x=list(range(1, 11)), y=wcss, title=\"Elbow Method for Optimal Clusters\", labels={\"x\": \"Number of Clusters\", \"y\": \"WCSS\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a66091",
   "metadata": {},
   "source": [
    "#### Apply K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de661d43",
   "metadata": {},
   "source": [
    "### What is K-Means Clustering?\n",
    "\n",
    "K-Means is an unsupervised machine learning algorithm used to group similar data points into clusters. It works by:\n",
    "\n",
    "1. Choosing a number of clusters (K).\n",
    "2. Randomly assigning data points to K clusters.\n",
    "3. Calculating the center (centroid) of each cluster.\n",
    "4. Reassigning each data point to the nearest centroid.\n",
    "5. Repeating steps 3 and 4 until the clusters no longer change.\n",
    "\n",
    "**Key Points:**\n",
    "- K-Means helps find patterns and groupings in data without using labels.\n",
    "- The \"elbow method\" is often used to choose the best number of clusters.\n",
    "- Useful for customer segmentation, grouping similar queries, and pattern discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f5cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df[\"Cluster\"] = kmeans.fit_predict(df[[\"Response_Time\", \"Satisfaction_Score\"]])\n",
    "\n",
    "# Interactive Scatter Plot for Clusters\n",
    "fig = px.scatter(df, x=\"Response_Time\", y=\"Satisfaction_Score\", color=\"Cluster\", title=\"Clusters of Queries\", labels={\"Response_Time\": \"Response Time\", \"Satisfaction_Score\": \"Satisfaction Score\", \"Cluster\": \"Cluster\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36709233",
   "metadata": {},
   "source": [
    "### Model Interpretability using Plotly and LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988059c7",
   "metadata": {},
   "source": [
    "### What is Model Interpretability?\n",
    "\n",
    "Model interpretability is the process of understanding and explaining how a machine learning model makes its predictions. It helps us answer questions like:\n",
    "\n",
    "- **Why did the model make this prediction?**\n",
    "- **Which features (inputs) were most important for the decision?**\n",
    "- **Can we trust the model‚Äôs results?**\n",
    "\n",
    "Interpretability is especially important in real-world applications where transparency, fairness, and trust are needed. Tools like LIME and interactive visualizations (e.g., with Plotly) help us see which factors influenced a specific prediction, making complex models easier to understand for everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92032cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretability with LIME\n",
    "explainer = LimeTabularExplainer(\n",
    "\ttraining_data=X_train.values,\n",
    "\tfeature_names=X_train.columns,\n",
    "\tclass_names=[str(c) for c in np.unique(y_train)],\n",
    "\tmode=\"classification\"\n",
    ")\n",
    "# Select a test instance to explain\n",
    "instance = X_test.iloc[0]\n",
    "\n",
    "exp = explainer.explain_instance(instance.values, clf.predict_proba, num_features=2)\n",
    "fig = exp.as_pyplot_figure()\n",
    "plt.show()  # Ensures the plot is displayed only once\n",
    "\n",
    "# Save the models\n",
    "import joblib\n",
    "joblib.dump(clf, \"random_forest_model.pkl\")\n",
    "joblib.dump(reg, \"linear_regression_model.pkl\")\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a151a",
   "metadata": {},
   "source": [
    "### What is RandomForestClassifier?\n",
    "\n",
    "`RandomForestClassifier` is a machine learning algorithm used for classification tasks. It works by building multiple decision trees and combining their results to make more accurate and stable predictions. Each tree is trained on a random subset of the data, and the final prediction is made by taking a majority vote from all the trees.\n",
    "\n",
    "**Key Points:**\n",
    "- Handles both numerical and categorical data.\n",
    "- Reduces overfitting compared to a single decision tree.\n",
    "- Works well with large datasets and many features.\n",
    "- Provides feature importance scores.\n",
    "\n",
    "**Typical Use Cases:**\n",
    "- Email spam detection\n",
    "- Medical diagnosis\n",
    "- Customer segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3684102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)  # classes: 0, 1, 2\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a new RandomForestClassifier on the Iris dataset\n",
    "clf_iris = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_iris.fit(X_train, y_train)\n",
    "\n",
    "# Ensure predict_proba returns proper output\n",
    "def modified_predict_proba(X_input):\n",
    "    proba = clf_iris.predict_proba(X_input)\n",
    "    if proba.shape[1] == 1:  # Handle single-class probability output\n",
    "        proba = np.hstack([proba, 1 - proba])\n",
    "    return proba\n",
    "\n",
    "# Initialize LIME Explainer\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=list(X_train.columns),\n",
    "    class_names=[\"Category_\" + str(cls) for cls in np.unique(y_train)],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Pick an instance to explain (for example, instance at index 5)\n",
    "i = 5\n",
    "instance = X_test.iloc[i].values  # 1-d array representing a single instance\n",
    "\n",
    "print(\"Instance to Explain:\", instance)\n",
    "\n",
    "# Generate the explanation using our modified_predict_proba function\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=instance,\n",
    "    predict_fn=modified_predict_proba,\n",
    "    num_features=4\n",
    ")\n",
    "# Print textual explanation\n",
    "print(exp.as_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01951617",
   "metadata": {},
   "source": [
    "### Interactive Query Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f044d",
   "metadata": {},
   "source": [
    "This section allows users to input their own student queries and instantly receive predictions about the query category, response flag, and estimated response time. By leveraging the trained machine learning model, the system provides real-time feedback and demonstrates how automated query classification can enhance student support services. Simply enter a sample query to see the model in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5725a4f",
   "metadata": {},
   "source": [
    "#### Below are sample queries that are ***present*** in the **dataset**. You can use these to test the model:\n",
    "\n",
    "- What is the deadline for fee payment?\n",
    "- How can I get a transcript?\n",
    "- I need help with course selection.\n",
    "- My grade has not been updated.\n",
    "- How do I register for courses?\n",
    "- When is the semester starting?\n",
    "- I cannot access my student portal.\n",
    "- Is there any scholarship available?\n",
    "- Website shows error 404.\n",
    "- I forgot my password, need assistance.\n",
    "\n",
    "***Feel free to use these queries to evaluate the prediction and classification capabilities of the model.*** \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ae500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # For response time calculation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd  # To handle query_data.csv\n",
    "\n",
    "# Define Category Mapping\n",
    "category_mapping = {\n",
    "    0: \"Academic\",\n",
    "    1: \"General\",\n",
    "    2: \"Technical\",\n",
    "    3: \"Financial\"\n",
    "}\n",
    "\n",
    "# Load Data from query_data.csv\n",
    "try:\n",
    "    query_data = pd.read_csv(\"query_data.csv\")  # Ensure the CSV contains columns 'Query_Text', 'Response_Flag'\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: query_data.csv file not found. Proceeding with example data.\")\n",
    "    query_data = pd.DataFrame({\n",
    "        \"Query_Text\": [\"I need help with my course syllabus\", \"What are the library timings?\", \n",
    "                       \"Website shows error 404\", \"What are the fees for admission?\"],\n",
    "        \"Response_Flag\": [\"Yes\", \"Yes\", \"No\", \"Yes\"]\n",
    "    })\n",
    "\n",
    "# Handle missing Response_Flag column\n",
    "if \"Response_Flag\" not in query_data.columns:\n",
    "    print(\"Response_Flag column found. Adding values.\")\n",
    "    query_data[\"Response_Flag\"] = [\"Yes\"] * len(query_data)  # Add default values\n",
    "\n",
    "# Training Data\n",
    "training_texts = query_data[\"Query_Text\"].values.tolist()\n",
    "\n",
    "# Generate labels that match the number of queries\n",
    "training_labels = [0, 1, 2, 3] * (len(training_texts) // 4)  # Repeat labels\n",
    "training_labels = training_labels[:len(training_texts)]  # Ensure labels match the number of samples\n",
    "\n",
    "# Initialize and Train the Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(training_texts)  # Train the vectorizer on training text\n",
    "\n",
    "# Transform Training Data\n",
    "text_features = vectorizer.transform(training_texts).toarray()\n",
    "satisfaction_scores = np.array([4.0] * len(training_texts)).reshape(-1, 1)  # Simulate satisfaction scores\n",
    "training_features = np.hstack((satisfaction_scores, text_features))  # Combine features\n",
    "\n",
    "# Check dimensions and assert correctness\n",
    "print(\"Shape of training features (X):\", training_features.shape)\n",
    "print(\"Length of training labels (y):\", len(training_labels))\n",
    "assert training_features.shape[0] == len(training_labels), \"Mismatch between features and labels!\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_features, training_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Retrain the classifier on the new features\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Predict Query Category\n",
    "def predict_query():\n",
    "    query_text = input(\"\\nEnter your student query: \")  # Get query input from user\n",
    "    satisfaction_score = np.array([4.0]).reshape(1, -1)  # Example score reshaped properly\n",
    "    query_text_feature = vectorizer.transform([query_text]).toarray()  # Vectorize input query\n",
    "    query_data_clf = np.hstack((satisfaction_score, query_text_feature))  # Combine features for prediction\n",
    "\n",
    "    # Simulate response time calculation\n",
    "    start_time = time.time()\n",
    "    predicted_category = clf.predict(query_data_clf)[0]  # Predict category\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time  # Calculate response time\n",
    "\n",
    "    # Use Category Mapping\n",
    "    category_name = category_mapping.get(predicted_category, \"Unknown\")\n",
    "    response_flag = query_data[query_data[\"Query_Text\"].str.lower() == query_text.lower()][\"Response_Flag\"].values\n",
    "    response_flag = response_flag[0] if len(response_flag) > 0 else \"No\"  # Default to \"No\" if no match\n",
    "\n",
    "    # Output Results\n",
    "    print(\"\\nüîç Prediction Result:\")\n",
    "    print(f\"Query: {query_text}\")\n",
    "    print(f\"Predicted Category: {category_name}\")\n",
    "    print(f\"Response Flag: {response_flag}\")\n",
    "    print(f\"Response Time: {response_time:.2f} seconds\")\n",
    "\n",
    "# Call the prediction function\n",
    "predict_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
